# backend/main.py
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from transformers import pipeline
import uuid
import random
import re
import time
import json
import logging

# Import the new prompts
import prompts

logging.basicConfig(level=logging.INFO)

# Initialize the Hugging Face pipeline for text generation with GPT-2
# Using a specific revision to ensure consistency
try:
    generator = pipeline("text-generation", model="gpt2")
except Exception as e:
    logging.error(f"Failed to load Hugging Face model: {e}")
    generator = None

# --- Static fallback questions (unchanged) ---
static_questions = {
    "easy": [
        {"text": "What is 2 + 2?", "options": ["2", "3", "4", "5"], "correct_index": 2},
        {"text": "Which planet is known as the Red Planet?", "options": ["Earth", "Mars", "Venus", "Jupiter"], "correct_index": 1}
    ],
    "medium": [
        {"text": "What is the square root of 81?", "options": ["7", "8", "9", "10"], "correct_index": 2},
        {"text": "In Python, what does len([10,20,30]) return?", "options": ["2", "3", "30", "Error"], "correct_index": 1}
    ],
    "hard": [
        {"text": "What is the time complexity of binary search?", "options": ["O(n)", "O(log n)", "O(n log n)", "O(1)"], "correct_index": 1},
        {"text": "Which algorithm is used in PageRank?", "options": ["K-means", "Gradient Descent", "Markov Chain", "Dijkstra"], "correct_index": 2}
    ]
}

# --- FastAPI Setup (unchanged) ---
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

sessions = {}

# --- Helper Functions (unchanged) ---
def _clean_choice(s: str) -> str:
    if not isinstance(s, str):
        s = str(s)
    s = s.strip()
    s = re.sub(r"^\s*([A-Da-d1-4])[\.\)]\s*", "", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def _normalize_unique(options):
    seen = set()
    out = []
    for o in options:
        c = _clean_choice(o)
        if c and c.lower() not in seen:
            out.append(c)
            seen.add(c.lower())
    return out

def _fallback(level="easy"):
    q = random.choice(static_questions[level]).copy()
    q["id"] = str(uuid.uuid4())
    q["difficulty"] = level
    q["skill"] = "General Knowledge"
    return q

# --- AI Interaction Functions (Updated) ---
def generate_question(level="easy", topic="math"):
    if not generator:
        logging.error("Generator pipeline not available. Using fallback.")
        return _fallback(level)

    prompt_text = prompts.get_question_prompt(level, topic)
    try:
        # Generate text and remove the prompt from the output
        full_output = generator(prompt_text, max_length=250, num_return_sequences=1, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']
        # Isolate only the text generated by the model
        raw_completion = full_output[len(prompt_text):].strip()
        
        logging.info(f"GPT-2 raw completion for question: {raw_completion}")

        json_match = re.search(r"\{.*\}", raw_completion, re.DOTALL)
        if not json_match:
            raise ValueError("No JSON object found in model completion")
        
        json_str = json_match.group(0)
        data = json.loads(json_str)
        
        text = data.get("text", "").strip()
        correct = _clean_choice(data.get("correct_answer", ""))
        skill = data.get("skill", topic)
        distractors = _normalize_unique(data.get("distractors", []))

        if not all([text, correct, len(distractors) >= 3]):
            raise ValueError("Missing or invalid fields from model output")

        options = _normalize_unique([correct] + distractors)[:4]
        random.shuffle(options)
        
        if correct not in options:
            options[random.randint(0, 3)] = correct

        return {
            "id": str(uuid.uuid4()),
            "text": text,
            "options": options,
            "correct_index": options.index(correct),
            "difficulty": level,
            "skill": skill
        }
    except Exception as e:
        logging.error(f"⚠️ GPT-2 question generation failed: {e}. Using fallback.")
        return _fallback(level)

def generate_learning_content(question_text, correct_answer):
    if not generator:
        logging.error("Generator pipeline not available. Using fallback explanation.")
        return f"Let's review this concept. The correct answer to '{question_text}' is **{correct_answer}**."
        
    prompt_text = prompts.get_learning_content_prompt(question_text, correct_answer)
    try:
        # Set max_length relative to the prompt length
        max_len = len(prompt_text.split(' ')) + 100
        
        full_output = generator(prompt_text, max_length=max_len, num_return_sequences=1, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text']
        # Isolate and clean the generated content
        explanation = full_output[len(prompt_text):].strip()
        
        # Stop at the first double newline to keep it concise
        explanation = explanation.split("\n\n")[0]
        
        logging.info(f"GPT-2 raw completion for content: {explanation}")
        
        if len(explanation.split()) < 5:
            raise ValueError("Generated explanation is too short.")

        return "Let's take a closer look at this. " + correct_answer + " is the correct answer because" + explanation

    except Exception as e:
        logging.error(f"⚠️ GPT-2 content generation failed: {e}")
        return f"Let's review this concept. The correct answer to '{question_text}' is **{correct_answer}**."

# --- Pydantic Models (unchanged) ---
class StartRequest(BaseModel):
    user_id: str
    topic: str

class AnswerRequest(BaseModel):
    session_id: str
    answer_index: int
    time_taken: float
    question_id: str | None = None

# --- Endpoints (Logic largely unchanged, just calls updated functions) ---
@app.post("/start")
def start_quiz(req: StartRequest):
    session_id = f"sess_{int(time.time())}"
    first_q = generate_question(level="easy", topic=req.topic)
    
    sessions[session_id] = {
        "user_id": req.user_id,
        "score": 0,
        "answered": 0,
        "level": "easy",
        "last_question": first_q,
        "topic": req.topic,
        "question_history": []
    }
    return {"session_id": session_id, "question": first_q, "progress": sessions[session_id]}

@app.get("/")
def welcome():
    return {"message": "Welcome to the Quiz API!"}

@app.post("/answer")
def submit_answer(req: AnswerRequest):
    session = sessions.get(req.session_id)
    if not session:
        return {"error": "session not found"}

    last_q = session.get("last_question")
    correct = False
    if last_q and 0 <= req.answer_index < len(last_q["options"]):
        correct = (req.answer_index == last_q["correct_index"])
    
    session["question_history"].append({
        "text": last_q["text"],
        "options": last_q["options"],
        "correct_index": last_q["correct_index"],
        "user_answer_index": req.answer_index,
        "is_correct": correct,
        "skill": last_q.get("skill", session.get("topic"))
    })

    explanation = ""

    if correct:
        session["score"] += 1
        next_level = "medium" if session["level"] == "easy" else "hard"
        session["level"] = next_level
        
        explanation = "Correct! ✅ Let's try something tougher."
        next_q = generate_question(level=session["level"], topic=session.get("topic", "math"))
        session["last_question"] = next_q
        next_step = {"type": "question", "data": next_q}

    else:
        next_level = "medium" if session["level"] == "hard" else "easy"
        session["level"] = next_level
        
        correct_answer_text = last_q["options"][last_q["correct_index"]]
        learning_content = generate_learning_content(last_q["text"], correct_answer_text)
        
        next_q = generate_question(level=session["level"], topic=session.get("topic", "math"))
        session["last_question"] = next_q
        
        next_step = {
            "type": "content", 
            "data": {
                "title": f"Reviewing: {last_q.get('skill', 'Concept')}",
                "content": learning_content,
                "next_question": next_q
            }
        }
        explanation = f"Not quite. The correct answer was **{correct_answer_text}**."

    session["answered"] += 1

    return {
        "correct": correct,
        "explanation": explanation,
        "correct_index": last_q["correct_index"],
        "next_step": next_step,
        "progress": session
    }

@app.get("/progress/{session_id}")
def get_progress(session_id: str):
    return sessions.get(session_id, {"error": "session not found"})